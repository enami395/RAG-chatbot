# SystÃ¨me RAG avec Routage Intelligent

Un systÃ¨me de chatbot basÃ© sur RAG (Retrieval-Augmented Generation) avec plusieurs stratÃ©gies de routage intelligent pour amÃ©liorer la qualitÃ© des rÃ©ponses.

## ğŸ“‹ Description

Ce projet implÃ©mente un systÃ¨me RAG avancÃ© qui utilise des techniques de routage pour diriger les questions vers le traitement le plus appropriÃ© :

1. **Routage Simple/Complexe** : DÃ©tecte si une question est simple ou complexe et adapte le traitement en consÃ©quence
2. **Routage Import/Export** : Route les questions vers le bon vector store selon qu'elles concernent l'import ou l'export

### FonctionnalitÃ©s

- âœ… Chargement et traitement de documents (DOCX)
- âœ… CrÃ©ation d'embeddings avec HuggingFace
- âœ… Stockage vectoriel avec ChromaDB
- âœ… Routage intelligent des questions
- âœ… DÃ©composition de questions complexes en sous-questions
- âœ… Support de plusieurs vector stores (import/export)

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8+
- Ollama (pour le LLM local)
- Jupyter Notebook

## ğŸ“ Structure du Projet

```
RAG Jupyter/
â”œâ”€â”€ rag_embedding_storing.ipynb    # CrÃ©ation des embeddings et vector stores
â”œâ”€â”€ rag_routing.ipynb              # SystÃ¨me de routage intelligent
â”œâ”€â”€ documents/                     # Dossier pour vos documents (Ã  crÃ©er)
â”‚   â”œâ”€â”€ import_document.docx
â”‚   â””â”€â”€ export_document.docx
â”œâ”€â”€ chroma_store/                  # Vector stores persistÃ©s (gÃ©nÃ©rÃ© automatiquement)
â”‚   â”œâ”€â”€ import/
â”‚   â””â”€â”€ export/
â”œâ”€â”€ .env.example                   # Exemple de fichier de configuration
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â””â”€â”€ README.md                      # Ce fichier
```

## ğŸ”§ Configuration

### 1. PrÃ©parer vos documents

Placez vos documents dans le dossier `documents/` :
- `documents/import_document.docx` : Documents relatifs Ã  l'import
- `documents/export_document.docx` : Documents relatifs Ã  l'export

### 2. CrÃ©er les vector stores

ExÃ©cutez le notebook `rag_embedding_storing.ipynb` pour :
- Charger vos documents
- Les dÃ©couper en chunks
- CrÃ©er les embeddings
- Persister les vector stores dans `chroma_store/`

**Important** : Modifiez les chemins des documents dans la cellule 2 du notebook avant d'exÃ©cuter.

### 3. Utiliser le systÃ¨me de routage

ExÃ©cutez le notebook `rag_routing.ipynb` pour utiliser le systÃ¨me de routage intelligent.

## ğŸ“– Utilisation

### Routage Simple/Complexe

Le systÃ¨me dÃ©tecte automatiquement si une question est simple ou complexe :

- **Questions simples** : TraitÃ©es directement avec un RAG standard
- **Questions complexes** : DÃ©composÃ©es en sous-questions, traitÃ©es individuellement, puis synthÃ©tisÃ©es

Exemple :
```python
result = hybrid_routing_pipeline("Quelles sont les exigences pour importer du matÃ©riel mÃ©dical au Maroc ?")
```

### Routage Import/Export

Le systÃ¨me route automatiquement les questions vers le bon vector store :

```python
result = import_export_router("Quels sont les documents nÃ©cessaires pour exporter du textile ?")
```

## ğŸ› ï¸ Technologies UtilisÃ©es

- **LangChain** : Framework pour les applications LLM
- **ChromaDB** : Base de donnÃ©es vectorielle
- **HuggingFace Embeddings** : ModÃ¨les d'embedding (BAAI/bge-large-en-v1.5)
- **Ollama** : LLM local (Mistral)
- **Jupyter Notebook** : Environnement de dÃ©veloppement

## âš™ï¸ ParamÃ¨tres Configurables

### Embeddings
- ModÃ¨le : `BAAI/bge-large-en-v1.5` (ou `BAAI/bge-base-en-v1.5` pour une version plus lÃ©gÃ¨re)
- Modifiable dans `rag_embedding_storing.ipynb`

### Text Splitting
- `chunk_size` : 1000 caractÃ¨res
- `chunk_overlap` : 100 caractÃ¨res
- SÃ©parateur personnalisÃ© pour les codes SH

### LLM
- ModÃ¨le : Mistral (via Ollama)
- Modifiable dans `rag_routing.ipynb`

## ğŸ“ Notes Importantes

1. **ModÃ¨les d'embedding** : Assurez-vous d'utiliser le mÃªme modÃ¨le d'embedding lors de la crÃ©ation des vector stores et lors de leur utilisation.

2. **Chemins des documents** : Modifiez les chemins dans `rag_embedding_storing.ipynb` pour pointer vers vos propres documents.

3. **Ollama** : Le systÃ¨me utilise Ollama pour le LLM local. Assurez-vous qu'Ollama est en cours d'exÃ©cution avant d'utiliser les notebooks.

4. **Vector stores** : Les vector stores sont persistÃ©s dans `chroma_store/`. Ne supprimez pas ce dossier si vous voulez rÃ©utiliser les embeddings.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## ğŸ“„ Licence

Ce projet est sous licence MIT.

## ğŸ‘¤ Auteur

CrÃ©Ã© dans le cadre d'un projet de stage.

## ğŸ™ Remerciements

- LangChain pour le framework
- HuggingFace pour les modÃ¨les d'embedding
- Ollama pour le LLM local
