{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418dd996",
   "metadata": {},
   "source": [
    "# Création des Embeddings et Vector Stores\n",
    "\n",
    "Ce notebook permet de créer et persister les embeddings de vos documents pour le système RAG.\n",
    "\n",
    "## Objectif\n",
    "\n",
    "- Charger vos documents (DOCX)\n",
    "- Les découper en chunks avec un séparateur personnalisé pour les codes SH\n",
    "- Créer les embeddings avec HuggingFace\n",
    "- Persister les vector stores dans ChromaDB (séparés par type : import/export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14482b5",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec047c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables d'environnement depuis un fichier .env\n",
    "# Créez un fichier .env avec vos clés API (voir ENV_SETUP.txt)\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration LangSmith (optionnel - pour le tracing)\n",
    "# Définissez LANGCHAIN_API_KEY dans votre fichier .env si vous souhaitez utiliser LangSmith\n",
    "if os.getenv(\"LANGCHAIN_API_KEY\"):\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Imports des bibliothèques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510d31c",
   "metadata": {},
   "source": [
    "## 3. Configuration et traitement des documents\n",
    "\n",
    "Cette section configure le modèle d'embedding, le text splitter, charge les documents, les découpe en chunks et crée les vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8a3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9e4463b74e4c4d97533b0c31176a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6b0938c4204a96a9613ba957d5afbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff56c5ea15ee421997acf5b19b992627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8c1f9c96bb461591024d332ec64d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eaad1e83556487a96908b8ed5a05f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4f5b6f884846eb9bf23a904cccf385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports nécessaires pour le traitement des documents et la création des embeddings\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Modèle d'embedding HuggingFace (BAAI/bge-large-en-v1.5 pour meilleure qualité)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Séparateur personnalisé pour les codes SH (Système Harmonisé)\n",
    "# Ce pattern détecte les codes SH dans les documents (ex: 1234.56, 123456)\n",
    "CODE_SH_SEPARATOR = r\"\\b(?:\\d{6,8}|\\d{4}(?:\\.\\d{2,4})?)\\b\"\n",
    "\n",
    "# Configuration du text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,          # Taille maximale d'un chunk\n",
    "    chunk_overlap=100,        # Chevauchement entre chunks pour préserver le contexte\n",
    "    add_start_index=True,     # Ajouter l'index de début dans les métadonnées\n",
    "    strip_whitespace=True,    # Supprimer les espaces en début/fin\n",
    "    separators=[CODE_SH_SEPARATOR],  # Utiliser le séparateur personnalisé\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Chargement des documents\n",
    "# ============================================================================\n",
    "# Remplacez ces chemins par les chemins vers vos propres documents\n",
    "# Format recommandé : placez vos documents dans un dossier \"documents/\" à la racine\n",
    "\n",
    "IMPORT_DOC_PATH = \"documents/import_document.docx\"  # Remplacez par votre chemin\n",
    "EXPORT_DOC_PATH = \"documents/export_document.docx\"  # Remplacez par votre chemin\n",
    "\n",
    "# Charger les documents\n",
    "loader_import = Docx2txtLoader(IMPORT_DOC_PATH)\n",
    "loader_export = Docx2txtLoader(EXPORT_DOC_PATH)\n",
    "\n",
    "docs_import = loader_import.load()\n",
    "docs_export = loader_export.load()\n",
    "\n",
    "print(f\"Documents import chargés : {len(docs_import)} pages\")\n",
    "print(f\"Documents export chargés : {len(docs_export)} pages\")\n",
    "\n",
    "# ============================================================================\n",
    "# Découpage en chunks\n",
    "# ============================================================================\n",
    "\n",
    "chunks_import = splitter.split_documents(docs_import)\n",
    "chunks_export = splitter.split_documents(docs_export)\n",
    "\n",
    "print(f\"Chunks import créés : {len(chunks_import)}\")\n",
    "print(f\"Chunks export créés : {len(chunks_export)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Ajout de métadonnées\n",
    "# ============================================================================\n",
    "# Les métadonnées permettent de filtrer et router les requêtes vers le bon vector store\n",
    "\n",
    "for chunk in chunks_import:\n",
    "    chunk.metadata[\"type\"] = \"import\"\n",
    "    \n",
    "for chunk in chunks_export:\n",
    "    chunk.metadata[\"type\"] = \"export\"\n",
    "\n",
    "# ============================================================================\n",
    "# Création et persistance des vector stores\n",
    "# ============================================================================\n",
    "\n",
    "# Créer les dossiers de stockage si nécessaire\n",
    "os.makedirs(\"chroma_store/import\", exist_ok=True)\n",
    "os.makedirs(\"chroma_store/export\", exist_ok=True)\n",
    "\n",
    "# Créer et persister le vector store pour les documents d'import\n",
    "vs_import = Chroma.from_documents(\n",
    "    chunks_import,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_store/import\",\n",
    "    collection_name=\"import_docs\"\n",
    ")\n",
    "vs_import.persist()\n",
    "\n",
    "# Créer et persister le vector store pour les documents d'export\n",
    "vs_export = Chroma.from_documents(\n",
    "    chunks_export,\n",
    "    embedding_model,\n",
    "    persist_directory=\"chroma_store/export\",\n",
    "    collection_name=\"export_docs\"\n",
    ")\n",
    "vs_export.persist()\n",
    "\n",
    "print(\"\\n✅ Embeddings et vector stores créés et persistés avec succès!\")\n",
    "print(\"   - Vector store import : chroma_store/import\")\n",
    "print(\"   - Vector store export : chroma_store/export\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b4b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
