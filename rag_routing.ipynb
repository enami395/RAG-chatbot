{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378751f4",
   "metadata": {},
   "source": [
    "# Système RAG avec Routage Intelligent\n",
    "\n",
    "Ce notebook implémente un système RAG (Retrieval-Augmented Generation) avec plusieurs stratégies de routage pour améliorer la qualité des réponses.\n",
    "\n",
    "## Routage par Type de Requête"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0bef4",
   "metadata": {},
   "source": [
    "Ce type de routage analyse la nature de la question de l'utilisateur pour choisir une chaîne de traitement adaptée.\n",
    "\n",
    "### Simple vs. Complexe \n",
    "\n",
    "**Mécanisme :** Un LLM classifie la question. \n",
    "- **Simple** : Répondable directement avec une seule requête sur les données. Utilise un RAG direct.\n",
    "- **Complexe** : Question à plusieurs étapes, plusieurs concepts ou comparaison. Le système décompose la question en sous-questions, les traite individuellement, puis synthétise une réponse finale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84571656",
   "metadata": {},
   "source": [
    "### Configuration initiale\n",
    "\n",
    "Cette cellule configure le LLM (Ollama/Mistral) et charge le vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b9fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c51454ca90a4f6eb17e1e8014bf9226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc539600acc04199be3018c24ffad102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8998b2f319542ff8f1323258e415359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1219a57091df418f8fe8a9162caaf023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9f43e632ab4d5eb000fbf84b7040d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2553e04918d14100b5740f180757a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fc11541fcd445d9e453aaac3350afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f788c821d20e4d5b9e84f4ce27c52e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc974e3149e4b958d11eb126d22194c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077c8c4af1b74e5ebb4261509c9c39c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74876622553f49999fb4a9741ff2505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Imports et Configuration Initiale\n",
    "# ============================================================================\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Configuration du LLM (Ollama avec modèle Mistral)\n",
    "# Assurez-vous qu'Ollama est installé et que le modèle mistral est téléchargé\n",
    "# Installation : https://ollama.ai/\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Modèle d'embedding (doit correspondre EXACTEMENT à celui utilisé lors de la création des vector stores)\n",
    "# IMPORTANT: Utilisez le même modèle que dans rag_embedding_storing.ipynb\n",
    "# Par défaut: BAAI/bge-large-en-v1.5 (ou BAAI/bge-base-en-v1.5 pour une version plus légère)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Charger le vector store (assurez-vous que les vector stores ont été créés avec rag_embedding_storing.ipynb)\n",
    "# Pour un vector store simple (sans séparation import/export)\n",
    "# Note: Si vous utilisez des vector stores séparés (import/export), voir la section suivante\n",
    "retriever = Chroma(\n",
    "    persist_directory=\"chroma_store\", \n",
    "    embedding_function=embedding_model\n",
    ").as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714ad87",
   "metadata": {},
   "source": [
    "### Implémentation du routage Simple/Complexe\n",
    "\n",
    "Cette section contient :\n",
    "- Les templates de prompts\n",
    "- Le classificateur de type de question\n",
    "- Le pipeline de décomposition pour questions complexes\n",
    "- La fonction principale de routage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30331ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Imports pour le Routage Intelligent\n",
    "# ============================================================================\n",
    "\n",
    "from typing import Literal\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ============================================================================\n",
    "# Templates de Prompts\n",
    "# ============================================================================\n",
    "\n",
    "# Template principal pour les réponses RAG\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Classificateur Simple vs Complexe\n",
    "# ============================================================================\n",
    "\n",
    "detection_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Tu es un assistant expert. Ta tâche est de déterminer si une question est simple ou complexe.\n",
    "- Simple : répondable directement avec une seule requête sur les données.\n",
    "- Complexe : question à plusieurs étapes, plusieurs concepts ou comparaison.\n",
    "\n",
    "Question : {question}\n",
    "Catégorie (simple ou complexe) :\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def detect_query_type(question: str) -> Literal[\"simple\", \"complexe\"]:\n",
    "    \"\"\"\n",
    "    Détermine si une question est simple ou complexe.\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        \n",
    "    Returns:\n",
    "        \"simple\" ou \"complexe\"\n",
    "    \"\"\"\n",
    "    output = (detection_prompt | llm | StrOutputParser()).invoke({\"question\": question})\n",
    "    if \"complexe\" in output.lower():\n",
    "        return \"complexe\"\n",
    "    return \"simple\"\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Pipeline de Décomposition pour Questions Complexes\n",
    "# ============================================================================\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant that generates multiple sub-questions related to an input question.\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation.\n",
    "Generate multiple search queries related to: {question}\n",
    "Output (3 queries):\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Chaîne pour générer les sous-questions\n",
    "generate_subqs = (\n",
    "    prompt_decomposition \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.strip().split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Template pour la fusion des réponses aux sous-questions\n",
    "fusion_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "Voici les réponses à chaque sous-question :\n",
    "{details}\n",
    "\n",
    "Répond maintenant à la question initiale :\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def format_history(history):\n",
    "    \"\"\"Formate l'historique de conversation en texte.\"\"\"\n",
    "    return \"\\n\".join(f\"{msg.type}: {msg.content}\" for msg in history)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Pipeline de Routage Intelligent\n",
    "# ============================================================================\n",
    "\n",
    "def hybrid_routing_pipeline(query: str):\n",
    "    \"\"\"\n",
    "    Pipeline principal qui route les questions vers le traitement approprié.\n",
    "    \n",
    "    - Questions simples : RAG direct\n",
    "    - Questions complexes : Décomposition en sous-questions puis synthèse\n",
    "    \n",
    "    Args:\n",
    "        query: La question de l'utilisateur\n",
    "        \n",
    "    Returns:\n",
    "        La réponse générée\n",
    "    \"\"\"\n",
    "    # Détecter le type de question\n",
    "    query_type = detect_query_type(query)\n",
    "    print(f\"Type de question détecté : {query_type}\")\n",
    "\n",
    "    # Chaîne RAG de base\n",
    "    runnable_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Traitement des questions simples\n",
    "    if query_type == \"simple\":\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        return runnable_chain.invoke({\n",
    "            \"context\": context, \n",
    "            \"question\": query,\n",
    "            \"history\": \"\"\n",
    "        })\n",
    "\n",
    "    # Traitement des questions complexes : décomposition\n",
    "    chat_history = InMemoryChatMessageHistory()\n",
    "    chat_history.add_user_message(query)\n",
    "    \n",
    "    # Générer les sous-questions\n",
    "    subqs = generate_subqs.invoke({\"question\": query})\n",
    "    print(f\"Sous-questions générées : {len(subqs)}\")\n",
    "\n",
    "    # Répondre à chaque sous-question\n",
    "    answers = []\n",
    "    for subq in subqs:\n",
    "        if not subq.strip():\n",
    "            continue\n",
    "        chat_history.add_user_message(subq)\n",
    "        docs = retriever.get_relevant_documents(subq)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        resp = runnable_chain.invoke({\n",
    "            \"context\": context, \n",
    "            \"question\": subq,\n",
    "            \"history\": \"\"\n",
    "        })\n",
    "        chat_history.add_ai_message(resp)\n",
    "        answers.append((subq, resp))\n",
    "\n",
    "    # Fusionner les réponses\n",
    "    details = \"\\n\\n\".join(f\"Q: {q}\\nA: {a}\" for q, a in answers)\n",
    "    history_text = format_history(chat_history.messages)\n",
    "\n",
    "    final_chain = fusion_template | llm | StrOutputParser()\n",
    "    final_answer = final_chain.invoke({\n",
    "        \"details\": details,\n",
    "        \"question\": query,\n",
    "        \"history\": history_text\n",
    "    })\n",
    "\n",
    "    chat_history.add_ai_message(final_answer)\n",
    "    return final_answer\n",
    "\n",
    "# ============================================================================\n",
    "# Exemple d'utilisation\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Quelles sont les exigences et les documents nécessaires pour importer du matériel médical temporairement au Maroc ?\"\n",
    "    result = hybrid_routing_pipeline(question)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RÉPONSE FINALE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3247ef0",
   "metadata": {},
   "source": [
    "### Configuration des vector stores Import/Export\n",
    "\n",
    "Charge les vector stores séparés pour l'import et l'export créés dans `rag_embedding_storing.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0836b73a",
   "metadata": {},
   "source": [
    "## Routage vers Import/Export Vector Store\n",
    "\n",
    "Cette section implémente un routage qui dirige les questions vers le bon vector store selon qu'elles concernent l'import ou l'export.\n",
    "\n",
    "**Mécanisme :** Un classificateur LLM détermine si la question concerne l'import ou l'export, puis route la requête vers le vector store approprié."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f91f5",
   "metadata": {},
   "source": [
    "### Implémentation du routage Import/Export\n",
    "\n",
    "Cette section contient :\n",
    "- Le classificateur Import/Export\n",
    "- La fonction de routage vers le bon vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration des Vector Stores Import/Export\n",
    "# ============================================================================\n",
    "# Note: Assurez-vous d'avoir créé les vector stores avec rag_embedding_storing.ipynb\n",
    "# avant d'exécuter cette cellule\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Modèle d'embedding (doit correspondre à celui utilisé lors de la création)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Charger les vector stores persistés\n",
    "vs_import = Chroma(\n",
    "    persist_directory=\"chroma_store/import\",\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=\"import_docs\"\n",
    ")\n",
    "\n",
    "vs_export = Chroma(\n",
    "    persist_directory=\"chroma_store/export\",\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=\"export_docs\"\n",
    ")\n",
    "\n",
    "# Créer les retrieveurs\n",
    "retriever_import = vs_import.as_retriever()\n",
    "retriever_export = vs_export.as_retriever()\n",
    "\n",
    "print(\"✅ Vector stores import/export chargés avec succès\")\n",
    "\n",
    "# ============================================================================\n",
    "# Classificateur Import/Export\n",
    "# ============================================================================\n",
    "\n",
    "classification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant qui doit déterminer si une question concerne l'import ou l'export.\n",
    "\n",
    "Catégorise cette question : \"{question}\"\n",
    "\n",
    "Réponds uniquement par : import ou export.\n",
    "\"\"\")\n",
    "\n",
    "def detect_import_or_export(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Détermine si une question concerne l'import ou l'export.\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        \n",
    "    Returns:\n",
    "        \"import\" ou \"export\"\n",
    "    \"\"\"\n",
    "    output = (classification_prompt | llm | StrOutputParser()).invoke({\"question\": question})\n",
    "    return \"import\" if \"import\" in output.lower() else \"export\"\n",
    "\n",
    "# ============================================================================\n",
    "# Router Import/Export\n",
    "# ============================================================================\n",
    "\n",
    "def import_export_router(question: str):\n",
    "    \"\"\"\n",
    "    Route une question vers le vector store approprié (import ou export).\n",
    "    \n",
    "    Args:\n",
    "        question: La question de l'utilisateur\n",
    "        \n",
    "    Returns:\n",
    "        La réponse générée à partir du bon vector store\n",
    "    \"\"\"\n",
    "    # Détecter le type de flux\n",
    "    target = detect_import_or_export(question)\n",
    "    print(f\"Type de flux détecté : {target}\")\n",
    "\n",
    "    # Sélectionner le bon retriever\n",
    "    retriever = retriever_import if target == \"import\" else retriever_export\n",
    "\n",
    "    # Récupérer les documents pertinents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Template de prompt pour la réponse\n",
    "    rag_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Réponds à la question suivante à partir des informations ci-dessous.\n",
    "\n",
    "CONTEXTE:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\"\"\"\n",
    "    )\n",
    "\n",
    "    # Générer la réponse\n",
    "    answer = (\n",
    "        {\"context\": context, \"question\": question} \n",
    "        | rag_prompt \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "    ).invoke()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ============================================================================\n",
    "# Exemple d'utilisation\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"Quels sont les documents nécessaires pour exporter du textile au Maroc ?\"\n",
    "    result = import_export_router(question)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RÉPONSE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
